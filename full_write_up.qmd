---
title: "Assignment 1 Full Writeup"
format: html
---

# Introduction and Literature Review

In the socio-political landscape, the manner and content of how leaders communicate provides critical insight into their governance style, priorities, and ideology. The State of the Nation Address (SONA) serves as an essential touchstone in South Africa's political calendar, where the sitting president not only provides an annual report on the nation's status but also sets the tone for policy directions and government intent for the subsequent year. Delivered at the commencement of a joint sitting of Parliament, particularly in election years, this address receives heightened scrutiny, given that it occurs twice: pre- and post-election [@sona2023].

Natural Language Processing (NLP) has been increasingly leveraged in the domain of political science to uncover patterns, biases, and ideologies in the speeches and writings of political leaders. Recent advancements in machine learning and NLP tools have enabled more refined text analysis, going beyond mere word frequency to semantic content and stylistic nuances. Researchers such as Katre [-@Katre2019] and Glavas, Nanni and Ponzetto [-@glavas-etal-2019-computational] have demonstrated the efficacy of using NLP to categorise and analyse political speeches. This raises the question: Can we discern, based purely on textual analysis, which South African president might have uttered a particular sentence during their SONA speech? In other words - can we predict the author of a sentence based on the content and style of the sentence?

However, while there is an abundance of literature on NLP applications in sentiment analysis and topic modelling, its application to discern between specific authors or speakers, especially in the South African political sphere, remains largely unexplored (although there has been some development with regards to creating text resources for South African languages [@Eiselen2014]). This gap is particularly noticeable when considering the unique linguistic, cultural, and political landscape of South Africa. The challenges lie not just in the variety of linguistic styles but also in the depth and breadth of topics covered, as well as the personal idiosyncrasies of each president (within a single speech as well as over time).

Given the above context, this paper aims to predict which of the South African presidents between 1994 and 2022 might have said a specific sentence during their SONA address. It leverages various text transformation techniques, such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (tf-idf), and text embeddings (a very simple embedding as well as BERT). Subsequent application of machine learning models, including a feed-forward neural net, Support Vector Machine (SVM), Naive Bayes, and a BERT classification model, offers a comparative lens to evaluate the efficacy of each approach.

# Data Preparation

The dataset [@sonadata2023] was comprised of a series of text files of the State of the Nation Addresses (SONA) from 1994 through 2022. Each speech's content was subsequently ingested, omitting the initial lines. These speeches were then collated into a structured format for more convenient access and manipulation.

Subsequently, essential metadata, including the year of the address and the name of the delivering president, were gleaned. Ater that, the removal of URLs, HTML character codes, and newline characters was performed. Additionally, the date of each address was extracted and appropriately formatted.

To achieve the project's objectives, each speech was dissected into its individual sentences. This granular breakdown facilitated the mapping of each sentence to its originating president. The finalised structured dataset comprises individual sentences paired with their respective presidents. This dataset was also saved as a csv file for future use.

For the model building, the data was prepared by create a 70-15-15 train-validation-test split, with the same seed being used for each method to ensure fair comparisons.

```{python}

# Loading in the necessary libraries
import zipfile
import os
import pandas as pd
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
from itertools import cycle
import seaborn as sns
import numpy as np
from collections import Counter
from nltk.tokenize import word_tokenize

```




```{python}

# Unzip the file and get the list of filenames
with zipfile.ZipFile("data/speeches.zip", 'r') as zip_ref:
    zip_ref.extractall("data")

filenames = os.listdir("data")
filenames = [filename for filename in filenames if filename.endswith('.txt')]

# Read the content of each speech file and extract the date from the first line
speeches = []
dates = []
for filename in filenames:
    with open(os.path.join("data", filename), 'r', encoding='utf-8') as file:
        # Extract date from the first line
        date = file.readline().strip()
        dates.append(date)
        
        # Read the rest of the file
        speeches.append(file.read())

# Create DataFrame
sona = pd.DataFrame({'filename': filenames, 'speech': speeches, 'date': dates})

# Extract year and president for each speech
sona['year'] = sona['filename'].str[:4]
sona['president'] = sona['filename'].str.split('_').str[-1].str.split('.').str[0]

# Clean the sona dataset by removing unnecessary text
replace_reg = r'(http.*?(\s|.$))|(www.*?(\s|.$))|&amp;|&lt;|&gt;|\n'
sona['speech'] = sona['speech'].str.replace(replace_reg, ' ')

# Split speeches into sentences
sona_sentences = sona['speech'].str.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', expand=True).stack().reset_index(level=-1, drop=True)
sona_sentences.name = 'sentence'

# Remove newline characters from the sentences
sona_sentences = sona_sentences.str.replace('\n', '').str.strip()

# Merge with the president, date, and year columns to associate each sentence with the respective details
df_sentences = sona[['president', 'date', 'year']].join(sona_sentences)

# Make a csv of the sentences
df_sentences.to_csv('data/sentences.csv', index=False)

```

## Number of speeches per president

```{python}

speeches_per_president = sona.groupby('president').size().reset_index(name='number_of_speeches')

# Display the table for number of speeches per president in a well-formatted manner
speeches_per_president.set_index('president', inplace=True)

# Plot the number of speeches per president
speeches_per_president.plot(kind='bar', legend=False, color='mediumpurple', edgecolor='black', ax=plt.gca())
plt.title('Number of Speeches per President')
plt.ylabel('Number of Speeches')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


```

The bar plot above illustrates the total number of speeches given by each president. Mbeki and Zuma had most speeches in the dataset, with 10 each. This means that there's a substantial amount of data available for them, which could be advantageous when discerning their linguistic patterns, given that there is not a significant overlap in the sentences of the two presidents. Motlanthe and de Klerk only had one speech each, which may be an issue, due to an imbalance in the data, which may bias the model output later. To explore this further, the number of sentences per president is examined.

## Number of sentences per president

```{python}

sentences_per_president = df_sentences.groupby('president').size().reset_index(name='number_of_sentences')
sentences_per_president.plot(x='president', y='number_of_sentences', kind='bar', legend=False, color='skyblue', edgecolor='black')
plt.title('Number of Sentences per President')
plt.ylabel('Number of Sentences')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

```


The plot above gives a breakdown of the number of sentences spoken by each president. Zuma stands out with the most sentences, further underscoring his prominence in the dataset. Notably, while Mbeki gave three more speeches than Ramaphosa, their sentence count is nearly the same, implying that Ramaphosa's speeches might be more verbose or detailed. This data provides a deeper understanding of the granularity of each president's contribution and reaffirms the potential data imbalance to be addressed in model development, especially when considering the fact that de Klerk and Motlanthe have less than 300 sentences each, while the others have well over 1500.

## Average sentence length per president


```{python}

df_sentences['sentence_length'] = df_sentences['sentence'].str.split().str.len()
avg_sentence_length = df_sentences.groupby('president')['sentence_length'].mean().reset_index()

avg_sentence_length.plot(x='president', y='sentence_length', kind='bar', legend=False, color='lightcoral', edgecolor='black')
plt.title('Average Sentence Length per President')
plt.ylabel('Average Sentence Length (words)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


```

This plot unveils the average sentence length, in words, for each president. A striking observation is that Zuma, despite having the most sentences and speeches, has a relatively concise average sentence length. Conversely, Mbeki and Motlanthe have longer average sentence lengths, with Mbeki being the only president that had over 30 words per sentence, on average. This metric offers insights into the verbosity and style of each president, which can be a useful feature when discerning speech patterns in model building.

## Word clouds for each president

```{python}

from itertools import cycle
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# Set the stopwords
stopwords = set(STOPWORDS)

# Selected colormaps
colormaps = cycle(['viridis', 'plasma', 'magma', 'cividis'])

# Adjusting the layout for the word clouds
plt.figure(figsize=(15, 30))

# Generate a word cloud for each president, with adjusted layout
for idx, (president, group) in enumerate(df_sentences.groupby('president')):
    text = ' '.join(group['sentence'])
    wc = WordCloud(stopwords=stopwords, background_color='white', colormap=next(colormaps), max_words=200, width=800, height=400).generate(text)
    
    # Display the word cloud
    plt.subplot(len(df_sentences['president'].unique()), 1, idx + 1)
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(president, fontdict={'fontsize': 20, 'fontweight': 'medium'})

plt.tight_layout()
plt.show()


```

The word clouds above offer a visually compelling representation of the most frequently used words by each president. The size of each word in the cloud corresponds to its frequency in the speeches. All the presidents had "will" as their most prominent word and referred to the country many times while speaking (highlighted by the use of the words "south" and "africa"/"african"). Motlanthe seemed to focus more on the economy and public image with the use of words such as "national", "public" and "government", whereas Mandela seemed to focus more on the people with the use of words such as "people" and "us". de Klerk focused more on the constitution and forming alliances during a transitional period, and Zuma focused more on work and the development. These word clouds provide a snapshot of the focal points and themes of each president's speeches. Distinctive words or terms can be potential features when building predictive models. The words from the wordclouds can also be seen in the bar plots below.

## Word frequency distribution for each president


```{python}

# Define a simple tokenizer function
def simple_tokenize(text):
    return [word for word in text.split() if word.isalpha()]

# Update the plotting function to use the simple tokenizer
def plot_word_frequency(text, president_name, n=10):
    tokens = simple_tokenize(text.lower())
    filtered_tokens = [word for word in tokens if word not in stopwords]
    frequency = Counter(filtered_tokens)
    most_common = frequency.most_common(n)
    
    words, counts = zip(*most_common)
    plt.bar(words, counts, color='lightseagreen')
    plt.title(f"Top {n} Words used by {president_name}")
    plt.xticks(rotation=45)
    plt.ylabel("Frequency")
    plt.show()

# Plot word frequency distribution for each president using the updated function
for president, group in df_sentences.groupby('president'):
    plot_word_frequency(' '.join(group['sentence']), president)



```


## N-gram frequency distributions for each president


### Bigrams

```{python}

# Define a function to generate n-grams
def generate_ngrams(text, n):
    tokens = simple_tokenize(text.lower())
    filtered_tokens = [word for word in tokens if word not in stopwords]
    ngrams = zip(*[filtered_tokens[i:] for i in range(n)])
    return [" ".join(ngram) for ngram in ngrams]

# Define function to plot N-gram frequency
def plot_ngram_frequency(text, president_name, n=2, top_n=10):
    ngrams = generate_ngrams(text, n)
    frequency = Counter(ngrams)
    most_common = frequency.most_common(top_n)
    
    phrases, counts = zip(*most_common)
    plt.bar(phrases, counts, color='lightsalmon')
    plt.title(f"Top {top_n} {n}-grams used by {president_name}")
    plt.xticks(rotation=45, ha='right')
    plt.ylabel("Frequency")
    plt.show()

# Plot bigram frequency distribution for each president
for president, group in df_sentences.groupby('president'):
    plot_ngram_frequency(' '.join(group['sentence']), president)


```


Instead of only looking at single word frequency, bigrams can also be used to find the most common two-word phrases. The bigrams above elucidate the distinctive linguistic patterns and thematic foci of each president, presenting opportunities for differentiation. For instance, President Mandela's frequently used bigrams, such as "South Africans" and "national unity," reflect his emphasis on nation-building and reconciliation during his tenure. In contrast, President Zuma's bigrams like "economic growth" suggest a policy-driven discourse concentrated on economic dynamics. However, there are potential pitfalls. Overlapping or common bigrams across presidents, such as generic terms or phrases prevalent in political discourse, could introduce ambiguity, potentially hindering the model's precision. Additionally, while President Ramaphosa's bigrams like "South Africa" are distinctly frequent, they are not uniquely attributable to him, as such phrases are likely universal across South African presidencies. 

## Trigrams

```{python}

# Plot trigram frequency distribution for each president
for president, group in df_sentences.groupby('president'):
    plot_ngram_frequency(' '.join(group['sentence']), president, n=3)


```


Expanding on the analysis of linguistic markers, trigrams offer insights into the most recurrent three-word sequences employed by each president. The trigram outputs above further refine our understanding of the unique verbal choices and thematic concerns of each leader. For instance, President Mandela's recurrent trigrams, such as "trade union movement", underscore his consistent focus on the working class of South Africa. Meanwhile, President Zuma's trigrams, such as "expaned public works" indicate a focus on the public sector as a whole. Conversely, the presence of generic or universally applicable trigrams, such as "state nation address", might pose challenges. These broadly-used trigrams, inherent to political addresses across presidencies, might dilute the distinctive features of individual presidents, complicating the model's task. Moreover, trigrams like "south africa will" from President Ramaphosa, although salient, are emblematic of speeches common to all presidents, making them less distinguishing. Thus, while trigrams can accentuate the nuances of each president's discourse, the model would benefit from discerning the balance between distinctiveness and generic trigram usage.


## Sentence similarity between presidents

```{python}
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.manifold import TSNE
from sklearn.feature_extraction.text import TfidfVectorizer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

def bow_x(data):
    # Extract relevant columns
    text_data = data['sentence']
    y = data['president']

    # Initialize a CountVectorizer for BOW representation
    vectorizer = CountVectorizer(lowercase=True, token_pattern=r"(?u)\b\w+\b", stop_words='english')

    # Fit and transform the text data
    X = vectorizer.fit_transform(text_data)

    # Create a DataFrame from the BOW representation
    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())

    return bow_df


def tf_idf(df):
    sentences = df['sentence'].tolist()

    # Create a TfidfVectorizer with stop words removal
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')

    # Fit and transform the sentences to compute TF-IDF values
    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)

    # Create a new dataframe with TF-IDF values
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

    return tfidf_df


def tokenize_text(text_data, labels, max_features=10000, maxlen=100):
    # Tokenization
    tokenizer = Tokenizer(num_words=max_features)
    tokenizer.fit_on_texts(text_data)
    sequences = tokenizer.texts_to_sequences(text_data)

    # Filter out sequences that have length 0
    seq_ok = [i for i, s in enumerate(sequences) if len(s) > 0]
    valid_labels = [labels.iloc[i] for i in seq_ok]

    # Padding sequences
    filtered_sequences = [sequences[i] for i in seq_ok]
    x_pad = pad_sequences(filtered_sequences, maxlen=maxlen)


    return x_pad, valid_labels, tokenizer

```


## Bag of Words (BOW) representation

```{python}

# Sample a subset of the data to alleviate memory issues
sample_df = df_sentences.sample(frac=0.4, random_state=1)

# 1. Encoding sentences using Bag of Words (BOW)
bow_encoded = bow_x(sample_df)
y_bow = sample_df['president']

# 2. Dimensionality reduction using t-SNE
tsne = TSNE(n_components=2, random_state=1)
bow_tsne = tsne.fit_transform(bow_encoded)

# 3. Visualise the results
plt.figure(figsize=(12, 8))
for president in sample_df['president'].unique():
    indices = [i for i, label in enumerate(y_bow) if label == president]
    plt.scatter(bow_tsne[indices, 0], bow_tsne[indices, 1], label=president, alpha=0.7)

plt.title('Visualisation of Sentences using Bag of Words (BOW)')
plt.legend()
plt.show()

```


The Bag-of-Words (BoW) visualisation above reveals a pronounced central cluster with substantial overlap across presidential sentences, indicating pervasive shared linguistic elements. This convergence towards common terms suggests that the BoW representation predominantly captures universal themes and terminologies characteristic of political discourse. Such patterns, while illuminating shared linguistic tendencies, underscore potential challenges in predictive modeling, with the BoW approach possibly lacking the granularity to detect distinctive linguistic markers for each president.

## TF-IDF representation

```{python}

# Encoding sentences using TF-IDF for the sample data
tfidf_encoded_sample = tf_idf(sample_df)

# Dimensionality reduction using t-SNE for the TF-IDF encoded data
tfidf_tsne_sample = tsne.fit_transform(tfidf_encoded_sample)

# Visualise the results for the TF-IDF encoded sample data
plt.figure(figsize=(12, 8))
for president in sample_df['president'].unique():
    indices = [i for i, label in enumerate(sample_df['president']) if label == president]
    plt.scatter(tfidf_tsne_sample[indices, 0], tfidf_tsne_sample[indices, 1], label=president, alpha=0.7)

plt.title('Visualisation of Sampled Sentences using TF-IDF')
plt.legend()
plt.show()


```

Using the TF-IDF representation, the visualization depicts a dominant central cluster, reaffirming the presence of overlapping linguistic constructs across presidential discourses. Unlike the BoW representation, the TF-IDF visualization lacks discernible smaller clusters, and data points appear more dispersed. This dispersion underscores the varied thematic undertones each president might have explored, but the pronounced overlap in the central region suggests that these thematic variations are not sufficiently distinct in the TF-IDF space to provide clear demarcations. The observed patterns emphasize the challenges inherent in solely relying on TF-IDF for capturing the unique linguistic nuances of each president.

## Tokenization with Padding representation

```{python}

# Encoding sentences using Tokenization with Padding for the sample data
x_pad_sample, valid_labels_sample, _ = tokenize_text(sample_df['sentence'], sample_df['president'])

# Dimensionality reduction using t-SNE for the Tokenized data
tokenized_tsne_sample = tsne.fit_transform(x_pad_sample)

# Visualise the results for the Tokenized sample data
plt.figure(figsize=(12, 8))
for president in sample_df['president'].unique():
    indices = [i for i, label in enumerate(valid_labels_sample) if label == president]
    plt.scatter(tokenized_tsne_sample[indices, 0], tokenized_tsne_sample[indices, 1], label=president, alpha=0.7)

plt.title('Visualisation of Sampled Sentences using Tokenization with Padding')
plt.legend()
plt.show()

```

Utilising tokenization with padding, the resultant visualization presents multiple clusters, indicating the method's ability to recognize shared linguistic constructs or thematic groupings within the dataset. Notably, the significant intermingling of presidents within these clusters underscores the shared nature of discourse patterns across different presidencies. The absence of a dominant central cluster, a divergence from the BoW and TF-IDF representations, alludes to a more nuanced and diverse sentence representation in the embedding space, potentially attributed to the emphasis on sentence structure inherent in the tokenization method.

# Methods

## 1. Text Representation Techniques

### a. Bag-of-Words (BoW)

The Bag-of-Words (BoW) representation is a simplistic yet effective method for text data representation. It hinges on representing text by its constituent words, disregarding their order. Here, each word operates as a feature, with the text being represented by a vector that denotes the frequency of each word [@vm2019implementation].

Formally, given a vocabulary $V$ comprising $N$ unique words, each document $d$ can be depicted as a vector $\mathbf{v}_d$ in $\mathbb{R}^N$ , where the i-th element $v_{d,i}$ denotes the frequency of the i-th word in the document:

$$ 
\mathbf{v}_d = [v_{d,1}, v_{d,2}, \ldots, v_{d,N}]
$$

The dataset was transformed into a BoW representation with each row corresponding to a sentence, and each column reflecting the frequency of a word in that sentence. The `CountVectorizer` class from the `sklearn.feature_extraction.text` module was employed for this task, with English stop words being excluded to filter out prevalent words that lack significant meaning, such as "and", "the", and "is" [@pedregosa2011scikit].

### b. Term Frequency-Inverse Document Frequency (TF-IDF)

Contrastingly, the TF-IDF representation scales the frequency of words based on their occurrence across all documents, ensuring that words appearing too frequently across documents (potentially bearing lesser discriminative importance) are assigned lower weights [@geeksforgeekstfidf, @monkeylearntfidf].

The term frequency (TF) of a word in a document is the raw count of that word in the document. The inverse document frequency (IDF) of a word is defined as:

$$
\text{IDF}(w) = \log \left( \frac{N}{1 + \text{count}(w)} \right)
$$

where $N$ signifies the total number of documents and $\text{count}(w)$ represents the number of documents containing the word $w$. The TF-IDF value for a word in a document is then the product of its TF and IDF values [@geeksforgeekstfidf, @monkeylearntfidf].

The `TfidfVectorizer` class from the `sklearn.feature_extraction.text` module was employed to transform our dataset into this representation [@pedregosa2011scikit].

### c. Text Embedding

For processing by deep learning models like neural networks, textual data was tokenized and converted into sequences of numbers. The `Tokenizer` class from the `keras.preprocessing.text` module was utilized for this purpose. Subsequently, sentences were padded with zeros using `pad_sequences` from the `keras.preprocessing.sequence` module to ensure uniform length [@chollet2015keras].

## 2. Model Architectures and Training

### a. Feed-Forward Neural Network

Feed-forward neural networks (FFNNs) are a subset of artificial neural networks characterized by acyclic connections between nodes. They encompass multiple layers: an input layer, several hidden layers, and an output layer [@rumelhart1986learning].

The architecture of the neural network employed in this study is delineated as follows:

- Input Layer: This layer harbors neurons equal to the number of features in the dataset (word counts for BoW and TF-IDF, sequence length for text embeddings). The Rectified Linear Unit (ReLU) activation function was utilized owing to its efficiency and capability to mitigate the vanishing gradient issue:

$$ 
f(x) = \max(0, x)
$$

- Hidden Layers: Several hidden layers were introduced, each utilizing He initialization, which is proficient for layers with ReLU activation. A dropout layer succeeded each hidden layer to curb overfitting by randomly nullifying a fraction of input units during each training update.

- Output Layer: This layer contains neurons equal to the number of classes (presidents, in our scenario). The softmax function was employed as the activation function, generating a probability distribution over the classes:

$$ 
\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

for $i = 1, \ldots, K$ and $\mathbf{z}$ is the input vector to the softmax function.

Training was conducted using the Adam optimization algorithm with a learning rate of 0.001. Adam is adept at training deep neural networks via computing adaptive learning rates for each parameter, leveraging moving averages of the parameter gradients and squared gradients.

The `EarlyStopping` and `ReduceLROnPlateau` callbacks were also enlisted. The former halts the training process if validation loss ceases to improve for a stipulated number of epochs, while the latter diminishes the learning rate if the validation loss reaches a plateau [@chollet2015keras].

### b. Support Vector Machine (SVM)

The Support Vector Machine (SVM) is a supervised learning algorithm suitable for both classification and regression tasks. It operates by identifying the optimal hyperplane that segregates a dataset into distinct classes. Provided a set of training examples, each labeled as belonging to one of two categories, the SVM training algorithm constructs a model that categorizes new examples into one of the two categories [@cortes1995support].

Mathematically, given labeled training data $(x_1, y_1), \ldots, (x_N, y_N)$ where $x_i$ belongs to $\mathbb{R}^D$ and $y_i$ is either 1 or -1 (indicating the class the input $x_i$ belongs to), SVM seeks the hyperplane defined by $w$ and $b$ that optimally separates the data points of the two classes [@cortes1995support]:

$$ 
y_i(w \cdot x_i + b) \geq 1
$$

The objective of SVM is to maximize the margin, which is the distance between the hyperplane and the nearest point from either class. The decision function is then given by:

$$ 
f(x) = \text{sign}(w \cdot x + b)
$$

### c. Naive Bayes Classifier

Naive Bayes is a probabilistic classifier predicated on Bayes' theorem with strong (naive) independence assumptions among features [@raschka2014naive]. Given a set of features $X = x_1, \ldots, x_n$ and a class variable $C$, Bayes' theorem states:

$$ 
P(C|X) = \frac{P(X|C) \times P(C)}{P(X)}
$$

The Naive Bayes classifier posits that the effect of a particular feature in a class is independent of other features. This simplification expedites computation, hence the term 'naive' [@raschka2014naive].

In our problem, the Naive Bayes classifier estimates the probability of a sentence belonging to each president's class based on the features (word frequencies for BoW or TF-IDF values). The sentence is then classified to the class (president) with the highest probability.

## 3. Model Evaluation

Evaluating the performance of machine learning models is paramount as it unveils the efficacy of the model and areas of potential improvement. Our evaluation paradigm leverages standard metrics including accuracy, precision, recall, and F1 score to quantify various facets of the model's predictions in a multi-class classification setting such as ours, where predictions could be true or false for multiple classes (presidents, in this case).

### a. Accuracy

Accuracy furnishes a broad overview of the model's performance and is calculated as the ratio of correct predictions to the total predictions:

$$ 
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}
$$

Nonetheless, in imbalanced datasets, accuracy could be misleading.

### b. Precision

Precision scrutinizes the model's positive predictions. Specifically, it computes the frequency at which the model correctly predicted a specific president out of all predictions for that president:

$$ 
\text{Precision (for a given president)} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}
$$

Where:

- True Positives (TP): The number of sentences correctly identified as belonging to that president.

- False Positives (FP): The number of sentences erroneously identified as belonging to that president, while they belong to a different one.

Precision is particularly crucial in scenarios where the cost of a false positive is high.

### c. Recall (or Sensitivity)

Recall evaluates how effectively the model identifies sentences from a specific president. It calculates the proportion of actual sentences from a president that the model correctly identified:

$$ 
\text{Recall (for a given president)} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
$$

Where:

- False Negatives (FN): The number of sentences that genuinely belong to a president but were misclassified as belonging to another.

Recall is vital in contexts where missing a true instance is significant.

### d. F1 Score

The F1 score is the harmonic mean of precision and recall, providing a balance between them. It achieves its best value at 1 (perfect precision and recall) and its worst at 0:

$$ 
\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

The F1 score is particularly useful when there is an uneven data distribution among classes.

These metrics were computed for each president in our dataset and then averaged (weighted by the number of true instances for each president) to derive a single value representing the overall model's performance. This approach ensures that the model's aptitude to predict less frequent classes (presidents with fewer sentences) is considered, rendering the evaluation more robust and representative of the model's true capabilities in a multi-class setting.

Moreover, the models were also assessed on separate training and test datasets. The training dataset is the learning corpus for the model, while the test dataset presents a fresh, unseen set of data points to gauge the model's generalization to new data. This separation is pivotal to ensure that the model doesn’t merely memorize the training data (overfitting), but discerns the underlying patterns determining which president uttered a given sentence.


# Results

```{python}
#| output: false

from keras.layers import Dropout, BatchNormalization
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from keras.initializers import he_normal
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import pickle
import os
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.preprocessing import LabelEncoder
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense
from keras.regularizers import l2
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import load_model
import json
#import cuml
from IPython.display import Image, display

```


```{python}
# Construct and train a neural network using the provided parameters.
def train_neural_network(layer_neurons, regularization_strength, training_epochs, dropout_probability=0.5):

    model = Sequential()

    # Define the input layer with specified initialization and regularization
    model.add(Dense(layer_neurons[0], activation='relu', input_dim=input_dim, kernel_regularizer=l2(regularization_strength), kernel_initializer=he_normal()))
    model.add(BatchNormalization())
    model.add(Dropout(dropout_probability))

    # Add hidden layers as specified
    for neurons in layer_neurons[1:]:
        model.add(Dense(neurons, activation='relu', kernel_regularizer=l2(regularization_strength), kernel_initializer=he_normal()))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_probability))

    # Add the output layer
    model.add(Dense(num_classes, activation='softmax'))

    # Define optimizer and compile the model
    optimizer = optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    # Specify callbacks for training
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, min_delta=0.0001),
        ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)
    ]

    # Train the model
    training_history = model.fit(X_train, y_train, epochs=training_epochs, validation_data=(X_val, y_val), callbacks=callbacks)

    # Evaluate the model's performance on training and testing datasets
    train_scores = model.evaluate(X_train, y_train, verbose=1)
    test_scores = model.evaluate(X_test, y_test, verbose=0)

    print(f'Accuracy on training data: {train_scores[1] * 100:.2f}%\nError on training data: {(1 - train_scores[1]) * 100:.2f}')
    print(f'Accuracy on test data: {test_scores[1] * 100:.2f}%\nError on test data: {(1 - test_scores[1]) * 100:.2f}')

    # Process model predictions for metric calculations
    predictions = model.predict(X_test)
    predicted_labels = np.argmax(predictions, axis=1)  
    true_labels = np.argmax(y_test, axis=1)

    # Calculate various metrics for model evaluation
    metrics = {
        'precision': precision_score(true_labels, predicted_labels, average='weighted'),
        'recall': recall_score(true_labels, predicted_labels, average='weighted'),
        'f1_score': f1_score(true_labels, predicted_labels, average='weighted')
    }

    return {
        'val_loss': training_history.history['val_loss'],
        'val_accuracy': training_history.history['val_accuracy'],
        'train_loss': training_history.history['loss'],
        'train_accuracy': training_history.history['accuracy'],
        'test_loss': test_scores[0],
        'test_accuracy': test_scores[1],
        **metrics,
        'model': model,
        'history': training_history
    }

```


```{python}

save_directory2 = "saved_files/assignment1_saved_results"

def save_model_params(output, base_filename):
    # If the output contains a Keras model
    if 'best_model' in output and hasattr(output['best_model'], 'save_weights'):
        print("Saving Keras model architecture and weights...")

        # Save the entire model (architecture + weights)
        output['best_model'].save(base_filename + '_full_model.h5')
        output['best_model'] = "MODEL_SAVED_SEPARATELY"
    else:
        print("No Keras model detected or model does not support weight saving.")

    # Serialize the modified dictionary
    with open(base_filename + '.pkl', 'wb') as file:
        pickle.dump(output, file)
    print(f"Data saved to {base_filename}.pkl")

def load_model_params(base_filename, model_architecture_func=None):
    # Deserialize the dictionary
    with open(base_filename + '.pkl', 'rb') as file:
        output = pickle.load(file)

    # If there's a placeholder for the Keras model
    if 'best_model' in output and output['best_model'] == "MODEL_SAVED_SEPARATELY":
        print("Loading Keras model architecture and weights...")

        # Load the entire model (architecture + weights)
        model = tf.keras.models.load_model(base_filename + '_full_model.h5')
        output['best_model'] = model
    else:
        print("No placeholder for Keras model detected in the loaded data.")

    return output

def save_results(results, save_directory):
    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    # Save Accuracy Plot
    if 'fig_acc' in results:
        results['fig_acc'].savefig(os.path.join(save_directory, 'fig_acc.png'))
        del results['fig_acc']

    # Save Confusion Matrix
    if 'fig_cm' in results:
        results['fig_cm'].savefig(os.path.join(save_directory, 'fig_cm.png'))
        del results['fig_cm']

    # Save the rest of the results
    with open(os.path.join(save_directory, 'results.pkl'), 'wb') as f:
        pickle.dump(results, f)

def load_results(load_directory):
    results = {}

    # Load Accuracy Plot
    if os.path.exists(os.path.join(load_directory, 'fig_acc.png')):
        results['fig_acc'] = os.path.join(load_directory, 'fig_acc.png')

    # Load Confusion Matrix
    if os.path.exists(os.path.join(load_directory, 'fig_cm.png')):
        results['fig_cm'] = os.path.join(load_directory, 'fig_cm.png')

    # Load the rest of the results
    if os.path.exists(os.path.join(load_directory, 'results.pkl')):
        with open(os.path.join(load_directory, 'results.pkl'), 'rb') as f:
            data = pickle.load(f)
        results.update(data)

    return results

# Helper function to style DataFrames
def style_df(df):
    numeric_cols = df.select_dtypes(include=['number']).columns
    format_dict = {col: "{:.3f}" for col in numeric_cols}
    return df.style.set_table_styles({
        '': [{'selector': '',
              'props': [('border', '1px solid black')]}]
    }).format(format_dict).hide()

# Mapping for column names
column_mapping = {
    'train_accuracy': 'Training Accuracy',
    'val_accuracy': 'Validation Accuracy',
    'test_accuracy': 'Test Accuracy',
    'Hyperparameter Value': 'Hyperparameter Value'
}

def display_accuracy_plot(loaded_results):
    if 'fig_acc' in loaded_results:
        display(Image(filename=loaded_results['fig_acc']))

def display_results_table(loaded_results):
    if 'results_table' in loaded_results:
        df = loaded_results['results_table'].rename(columns=column_mapping)
        display(style_df(df))

def display_table_nn(loaded_results):
    if 'table_nn' in loaded_results:
        df = loaded_results['table_nn'].rename(columns=column_mapping)
        display(style_df(df))

def display_table_others(loaded_results):
    if 'table_others' in loaded_results:
        table = loaded_results['table_others'].rename(columns=column_mapping)
        # Extracting best row based on highest validation accuracy
        best_row = table[table['Validation Accuracy'] == table['Validation Accuracy'].max()]
        
        # Creating a dataframe for neat display
        best_df = pd.DataFrame({
            'Hyperparameter Value': best_row.iloc[0, 0],
            'Training Accuracy': best_row['Training Accuracy'].values,
            'Validation Accuracy': best_row['Validation Accuracy'].values,
            'Test Accuracy': best_row['Test Accuracy'].values
        })
        
        display(style_df(best_df))
        display(style_df(table))

def display_confusion_matrix(loaded_results):
    if 'fig_cm' in loaded_results:
        display(Image(filename=loaded_results['fig_cm']))

def display_test_classification_report(loaded_results):
     # Extracting and formatting values from the loaded_results dictionary
    precision_val = "{:.3f}".format(loaded_results['test_report']['precision']).rstrip('0').rstrip('.')
    recall_val = "{:.3f}".format(loaded_results['test_report']['recall']).rstrip('0').rstrip('.')
    f1_score_val = "{:.3f}".format(loaded_results['test_report']['f1_score']).rstrip('0').rstrip('.')

    # Creating a 2D array with headers and values
    data = [
        ['Metric', 'Value'],
        ['Precision', precision_val],
        ['Recall', recall_val],
        ['F1 Score', f1_score_val]
    ]

    # Creating the DataFrame without index and displaying it
    manual_df = pd.DataFrame(data[1:], columns=data[0])
    display(manual_df.style.hide())



```


```{python}

# Prepare dataset by splitting into training, validation, and test sets. Additionally, encode and transform labels into one-hot vectors.
def prepare_dataset(features, labels, split_ratios=(0.3, 0.5), random_seed=1):

    # Split data into training and a combined test+validation set with stratification
    X_train, X_combined, y_train, y_combined = train_test_split(features, labels, test_size=split_ratios[0], stratify=labels, random_state=random_seed)

    # Further split the combined set into separate test and validation sets
    X_test, X_val, y_test, y_val = train_test_split(X_combined, y_combined, test_size=split_ratios[1], stratify=y_combined, random_state=random_seed)

    # Convert dataframes to arrays (if they aren't already)
    X_train, X_val, X_test = X_train.values, X_val.values, X_test.values

    # Initialize label encoder and encode the labels to integers
    encoder = LabelEncoder()
    y_train_encoded = encoder.fit_transform(y_train)
    y_val_encoded = encoder.transform(y_val)
    y_test_encoded = encoder.transform(y_test)

    # Convert integer labels to one-hot vectors
    y_train_onehot = to_categorical(y_train_encoded)
    y_val_onehot = to_categorical(y_val_encoded)
    y_test_onehot = to_categorical(y_test_encoded)

    # Extract dimensions for input features and number of classes
    input_dim = X_test.shape[1]
    num_classes = y_test_onehot.shape[1]

    return {
        'X_train': X_train,
        'X_val': X_val,
        'X_test': X_test,
        'y_train': y_train_onehot,
        'y_val': y_val_onehot,
        'y_test': y_test_onehot,
        'input_dim': input_dim,
        'num_classes': num_classes
    }

```

```{python}

# An older function from when I had many separate data preparation methods
def prepare_data(data_preparation_func, x, y):
    data = data_preparation_func(x, y)
    return data


def hyperparameter_search_nn(seed, neural_net_func, X_train, y_train, X_val, y_val, X_test, y_test):
    np.random.seed(seed)

    neurons_space = [[800, 300], [800, 300, 100]]
    l2_reg_space = [0, 0.001, 0.01]
    dropout_space = [0.2, 0.4]
    num_epochs_space = [20]

    best_val_accuracy = 0
    best_params = {}
    best_model = None
    best_train_accuracies = []
    best_val_accuracies = []

    results_list = []

    for neurons in neurons_space:
        for l2_reg in l2_reg_space:
            for dropout_rate in dropout_space:
                for epochs in num_epochs_space:
                    print(f"Training with: neurons={neurons}, l2_reg={l2_reg}, dropout={dropout_rate}, epochs={epochs}")

                    results = neural_net_func(layer_neurons=neurons,
                                              regularization_strength=l2_reg,
                                              training_epochs=epochs,
                                              dropout_probability=dropout_rate)

                    results_list.append({
                        'neurons': str(neurons),
                        'l2_reg': l2_reg,
                        'dropout_rate': dropout_rate,
                        'epochs': epochs,
                        'train_accuracy': results['train_accuracy'][-1],
                        'val_accuracy': results['val_accuracy'][-1],
                        'test_accuracy': results['test_accuracy']
                    })

                    if results['val_accuracy'][-1] > best_val_accuracy:
                        best_val_accuracy = results['val_accuracy'][-1]
                        best_model = results['model']
                        history = results['history']
                        best_params = {
                            'neurons': neurons,
                            'l2_reg': l2_reg,
                            'dropout_rate': dropout_rate,
                            'epochs': epochs,
                            'val_accuracy': best_val_accuracy
                        }
                        best_train_accuracies = results['train_accuracy']
                        best_val_accuracies = results['val_accuracy']

    results_df = pd.DataFrame(results_list)

    output = {
        'best_train_accuracies': best_train_accuracies,
        'best_val_accuracies': best_val_accuracies,
        'best_model': best_model,
        'results_df': results_df,
        'history': history
    }

    return output

def train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed):
    scaler = cuml.preprocessing.StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    y_train_int = np.argmax(y_train, axis=1)
    y_val_int = np.argmax(y_val, axis=1)
    y_test_int = np.argmax(y_test, axis=1)

    best_C = 1
    best_val_accuracy = 0
    C_values = [0.001, 0.01, 0.1, 0.5, 1, 10, 100]
    results_list = []

    for C in C_values:
        svm_clf = cuml.svm.SVC(C=C, class_weight='balanced', random_state=seed, max_iter=1000)
        svm_clf.fit(X_train_scaled, y_train_int)
        y_val_pred = svm_clf.predict(X_val_scaled)
        val_accuracy = cuml.metrics.accuracy_score(y_val_int, y_val_pred)

        results_list.append({
            'C': C,
            'train_accuracy': cuml.metrics.accuracy_score(y_train_int, svm_clf.predict(X_train_scaled)),
            'val_accuracy': val_accuracy,
            'test_accuracy': cuml.metrics.accuracy_score(y_test_int, svm_clf.predict(X_test_scaled))
        })

        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            best_C = C

    best_svm_clf = cuml.svm.SVC(C=best_C, class_weight='balanced', random_state=seed, max_iter=1000)
    best_svm_clf.fit(X_train_scaled, y_train_int)

    results_df = pd.DataFrame(results_list)

    output = {
        'best_model': best_svm_clf,
        'results_df': results_df
    }

    return output

def train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test):
    param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}
    results_list = []

    for alpha in param_grid['alpha']:
        nb_clf = MultinomialNB(alpha=alpha)
        nb_clf.fit(X_train, np.argmax(y_train, axis=1))
        
        results_list.append({
            'alpha': alpha,
            'train_accuracy': accuracy_score(np.argmax(y_train, axis=1), nb_clf.predict(X_train)),
            'val_accuracy': accuracy_score(np.argmax(y_val, axis=1), nb_clf.predict(X_val)),
            'test_accuracy': accuracy_score(np.argmax(y_test, axis=1), nb_clf.predict(X_test))
        })

    results_df = pd.DataFrame(results_list)
    best_alpha = results_df.loc[results_df['val_accuracy'].idxmax(), 'alpha']
    best_nb_clf = MultinomialNB(alpha=best_alpha)
    best_nb_clf.fit(X_train, np.argmax(y_train, axis=1))

    output = {
        'best_model': best_nb_clf,
        'results_df': results_df
    }

    return output

```



```{python}

def plot_accuracies(title, x_label, x_data, train_accuracies, val_accuracies):
    plt.figure(figsize=(10, 6))
    plt.plot(x_data, train_accuracies, '-o', label='Training Accuracy', color='blue')
    plt.plot(x_data, val_accuracies, '-o', label='Validation Accuracy', color='red')
    plt.title(title)
    plt.xlabel(x_label)
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    return plt.gcf()

# A helper function I defined earlier as a relic to make other parts work
def display_results_table(df):
    display(df)


def plot_confusion_matrix(y_true, y_pred, title, class_labels=None):
    # Convert one-hot encoded labels to label encoded format if necessary
    if len(y_true.shape) > 1 and y_true.shape[1] > 1:
        y_true = np.argmax(y_true, axis=1)
    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:
        y_pred = np.argmax(y_pred, axis=1)

    cm = confusion_matrix(y_true, y_pred, labels=class_labels)
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title(title)
    return plt.gcf()


def visualize_and_verify_results(output, X_val, y_val, X_test, y_test, classifier_type, class_labels=None):

    results = {}

    # Plot accuracies
    if classifier_type == 'nn':
        fig_acc = plot_accuracies("Neural Network Accuracies",
                                  "Epochs",
                                  list(range(1, len(output['best_train_accuracies']) + 1)),
                                  output['best_train_accuracies'],
                                  output['best_val_accuracies'])
        results['fig_acc'] = fig_acc
    elif classifier_type in ['svm', 'nb']:
        param_key = 'C' if classifier_type == 'svm' else 'alpha'
        fig_acc = plot_accuracies(f"{classifier_type.upper()} Accuracies",
                                  param_key,
                                  output['results_df'][param_key],
                                  output['results_df']['train_accuracy'],
                                  output['results_df']['val_accuracy'])
        results['fig_acc'] = fig_acc

    # Display results table
    if classifier_type == 'nn':
        metrics = {
            'train_accuracy': output['best_train_accuracies'][-1],
            'val_accuracy': output['best_val_accuracies'][-1]
        }
        results['table_nn'] = pd.DataFrame([metrics])
    else:
        results['table_others'] = output['results_df']

    # Plot confusion matrices and generate reports
    if 'best_model' in output:
        if classifier_type == 'nn':
            y_pred = output['best_model'].predict(X_test)
            y_pred_classes = np.argmax(y_pred, axis=1)
        else:
            y_pred_classes = output['best_model'].predict(X_test)

        y_true_classes = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 and y_test.shape[1] > 1 else y_test
        fig_cm = plot_confusion_matrix(y_true_classes,
                                       y_pred_classes,
                                       f"Test Confusion Matrix - {classifier_type.upper()}",
                                       class_labels)
        results['fig_cm'] = fig_cm

        # Classification reports
        precision = precision_score(y_true_classes, y_pred_classes, average='weighted')
        recall = recall_score(y_true_classes, y_pred_classes, average='weighted')
        f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')

        results['test_report'] = {
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }

    return results

```


```{python}

# Convert the sentences to BoW representation
def bow(data):
    # Extract the text column from the input data
    text_data = data['sentence']

    # Initialize a CountVectorizer for BOW representation with specified preprocessing steps
    vectorizer = CountVectorizer(lowercase=True, token_pattern=r"(?u)\b\w+\b", stop_words='english')

    # Convert text data to BOW representation
    bow_matrix = vectorizer.fit_transform(text_data)

    # Convert BOW matrix to a DataFrame for better readability
    bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())

    return bow_df

# Convert the sentences to TF-IDF representation
def tf_idf(data):
    # Extract sentences from the dataframe
    sentences = data['sentence'].tolist()

    # Initialize a TfidfVectorizer with specified preprocessing steps
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')

    # Convert sentences to TF-IDF representation
    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)

    # Convert TF-IDF matrix to a DataFrame for better readability
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

    return tfidf_df

# Tokenize the input text data and pad the sequences to a fixed length.
def tokenize_text(text_data, labels, max_features=10000, maxlen=100):
    # Initialize and fit the tokenizer on the text data
    tokenizer = Tokenizer(num_words=max_features)
    tokenizer.fit_on_texts(text_data)
    sequences = tokenizer.texts_to_sequences(text_data)

    # Filter out sequences with length 0
    valid_indices = [i for i, s in enumerate(sequences) if len(s) > 0]
    valid_labels = [labels.iloc[i] for i in valid_indices]
    valid_sequences = [sequences[i] for i in valid_indices]

    # Pad sequences to the specified maximum length
    padded_sequences = pad_sequences(valid_sequences, maxlen=maxlen)

    return padded_sequences, valid_labels, tokenizer

```


```{python}

data_path = "data/sentences.csv"
sentence_data = pd.read_csv(data_path)

# Drop rows where 'sentence' is NaN
sentence_data = sentence_data.dropna(subset=['sentence'])

# Get the names of the two presidents with the lowest sentence counts
remove_presidents = sentence_data['president'].value_counts().tail(2).index.tolist()

# Filter the data to exclude sentences from these two presidents
sentence_data = sentence_data[~sentence_data['president'].isin(remove_presidents)]

# Set the seed
seed = 1

# Set the save directory of the models
save_directory = "saved_files/assignment1_savedvals"

```

## Bag of Words

In the exploration of various models on a training set employing a bag of words representation, distinct performance disparities were observed. Firstly, utilising a feed-forward neural network, an impressive training accuracy of 0.99 was attained. However, its validation accuracy registered at 0.6, hinting at potential overfitting. An analysis of the test set predictions, as depicted by the confusion matrix, demonstrated that the correct classes were predominantly predicted for the corresponding sentences. Notably, the test set metrics were: precision at 0.595, recall at 0.569, and the f1 score also at 0.569.

The employment of support vector machines (SVM) with the bag of words approach, post-tuning, yielded training and validation accuracies of 0.989 and 0.533 respectively, with the test accuracy being 0.55. The precision, recall, and f1 scores for this model stood at 0.551, 0.55, and 0.547 respectively.

Lastly, when Naive Bayes was paired with the bag of words method, post-optimisation, it achieved training and validation accuracies of 0.89 and 0.592 respectively. This model appeared less prone to overfitting compared to its counterparts. For the test set, precision was measured at 0.624, recall at 0.615, and interestingly, the test accuracy was noted to be 0.616, slightly higher than the recall.

```{python}

x = bow(sentence_data)
y = sentence_data['president']

# Prepare data
data = prepare_data(prepare_dataset, x, y)
X_train, y_train, X_val, y_val, X_test, y_test, input_dim, num_classes = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['X_test'], data['y_test'], data['input_dim'], data['num_classes']

# Scaled data for SVM
# scaler = cuml.preprocessing.StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_val_scaled = scaler.transform(X_val)
# X_test_scaled = scaler.transform(X_test)

# y_train_int = np.argmax(y_train, axis=1)
# y_val_int = np.argmax(y_val, axis=1)
# y_test_int = np.argmax(y_test, axis=1)

```



### Neural network

```{python}

# with tf.device('/device:GPU:0'):
#  nn_bow_tune = hyperparameter_search_nn(seed,
#                                        train_neural_network,
#                                        X_train, y_train,
#                                        X_val, y_val,
#                                        X_test, y_test)

# # Saving the model parameters
# save_model_params(nn_bow_tune, os.path.join(save_directory, 'nn_bow_tune'))

```

```{python}

# Load the tuned output
# nn_bow_tuned = load_model_params(os.path.join(save_directory, 'nn_bow_tune'))

# visualize_and_verify_results(nn_bow_tuned, X_val, y_val, X_test, y_test, 'nn', class_labels=None)

```


```{python}

save_path = save_directory2 + "/bow_nn_results"
# save_results(bow_nn_results, save_path)
loaded_results = load_results(save_path)

#display_loaded_results(loaded_results)

```

```{python}
#| label: nn bow accuracy plot
#| fig-cap: "An accuracy plot for the neural network model trained on the Bag of Words representation."

display_accuracy_plot(loaded_results)

```


```{python}
#| label: nn bow nn accuracies for the best fitted model
#| tbl-cap: "A table of accuracies for the neural network model trained on the Bag of Words representation."

display_table_nn(loaded_results)

```


```{python}
#| label: nn bow table
#| tbl-cap: "A table of results for the neural network model trained on the Bag of Words representation."

display_table_others(loaded_results)

```


```{python}
#| label: nn bow confusion matrix
#| fig-cap: "A confusion matrix for the neural network model trained on the Bag of Words representation."

display_confusion_matrix(loaded_results)

```


```{python}
#| label: nn bow test classification report
#| tbl-cap: "A table of test classification results for the neural network model trained on the Bag of Words representation."

display_test_classification_report(loaded_results)

```


### Support Vector Machine

```{python}

# svm_results_bow = train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed)

# # Save the model parameters
# save_model_params(svm_results_bow, os.path.join(save_directory, 'svm_bow_results'))

# # Load the saved model parameters
# svm_results_bow = load_model_params(os.path.join(save_directory, 'svm_bow_results'))

# results = visualize_and_verify_results(svm_results_bow, X_val_scaled, y_val_int, X_test_scaled, y_test_int, 'svm', class_labels=None)

# Save and load the results for later

save_path = save_directory2 + "/bow_svm_results"
# save_results(results, save_path)
loaded_results = load_results(save_path)
#display_loaded_results(loaded_results)

```

```{python}
#| label: svm bow accuracy plot
#| fig-cap: "An accuracy plot for the SVM model trained on the Bag of Words representation."

display_accuracy_plot(loaded_results)

```


```{python}
#| label: svm bow tuning table
#| tbl-cap: "A table of hyperparameter tuning results for the SVM model trained on the Bag of Words representation, with the best hyperparameters highlighted above."

display_table_others(loaded_results)

```


```{python}
#| label: svm bow confusion matrix
#| fig-cap: "A confusion matrix for the SVM model trained on the Bag of Words representation."

display_confusion_matrix(loaded_results)

```


```{python}
#| label: svm bow test classification report
#| tbl-cap: "A table of test classification results for the SVM model trained on the Bag of Words representation."

display_test_classification_report(loaded_results)

```


### Naive Bayes

```{python}

# nb_results_bow = train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test)

# # Save the model parameters
# save_model_params(nb_results_bow, os.path.join(save_directory, 'nb_bow_results'))

# # Load the saved model parameters
# nb_results_bow = load_model_params(os.path.join(save_directory, 'nb_bow_results'))

# results = visualize_and_verify_results(nb_results_bow, X_val, y_val, X_test, y_test, 'nb', class_labels=None)

# Save and load the results for later

save_path = save_directory2 + "/bow_nb_results"
# save_results(results, save_path)
loaded_results = load_results(save_path)
#display_loaded_results(loaded_results)

```

```{python}
#| label: nb bow accuracy plot
#| fig-cap: "An accuracy plot for the NB model trained on the Bag of Words representation."

display_accuracy_plot(loaded_results)

```


```{python}
#| label: nb bow tuning table
#| tbl-cap: "A table of hyperparameter tuning results for the NB model trained on the Bag of Words representation, with the best hyperparameters highlighted above."

display_table_others(loaded_results)

```


```{python}
#| label: nb bow confusion matrix
#| fig-cap: "A confusion matrix for the NB model trained on the Bag of Words representation."

display_confusion_matrix(loaded_results)

```


```{python}
#| label: nb bow test classification report
#| tbl-cap: "A table of test classification results for the NB model trained on the Bag of Words representation."

display_test_classification_report(loaded_results)

```


## TF-IDF

In a subsequent analysis utilising the term frequency-inverse document frequency (tf-idf) representation with various models, certain resemblances to the bag of words (bow) results were discerned. Firstly, with the feed-forward neural network, the accuracy plot bore a striking similarity to its bow counterpart. This model achieved a training accuracy of 0.99 and a validation accuracy of 0.588. The confusion matrix for test set predictions indicated that the majority of sentences were assigned their correct classes. The test set metrics recorded were: precision at 0.598, recall at 0.597, and the f1 score at 0.595.

When the support vector machines (SVM) were employed in tandem with the tf-idf representation, the accuracy plot was found to mirror that of the bow version. After tuning, the training accuracy registered at 0.968, with validation and test accuracies being 0.542 and 0.574, respectively. The precision, recall, and f1 scores for this model were 0.58, 0.574, and 0.573 in that order.

Lastly, the Naive Bayes model with the tf-idf approach displayed accuracy plots bearing a resemblance to the bow version. Post-optimisation, it yielded training, validation, and test accuracies of 0.915, 0.594, and 0.611 respectively. The precision and recall both stood at 0.611, whilst the test accuracy was slightly lower at 0.609.

```{python}

# import idf data
x = tf_idf(sentence_data)
y = sentence_data['president']

# Prepare the data
data = prepare_data(prepare_dataset, x, y)
X_train, y_train, X_val, y_val, X_test, y_test, input_dim, num_classes = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['X_test'], data['y_test'], data['input_dim'], data['num_classes']

# Scaled data for SVM
# scaler = cuml.preprocessing.StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_val_scaled = scaler.transform(X_val)
# X_test_scaled = scaler.transform(X_test)

# y_train_int = np.argmax(y_train, axis=1)
# y_val_int = np.argmax(y_val, axis=1)
# y_test_int = np.argmax(y_test, axis=1)

```

### Neural network

```{python}

# with tf.device('/device:GPU:0'):
#   nn_tf_tune = hyperparameter_search_nn(seed,
#                                         train_neural_network,
#                                         X_train, y_train,
#                                         X_val, y_val,
#                                         X_test, y_test)

# # Saving the model parameters
# save_model_params(nn_tf_tune, os.path.join(save_directory, 'nn_tf_results'))

# # Load the tuned output
# nn_tf_tune = load_model_params(os.path.join(save_directory, 'nn_tf_results'))

# results = visualize_and_verify_results(nn_tf_tune, X_val, y_val, X_test, y_test, 'nn', class_labels=None)

# Save and load the results for later

save_path = save_directory2 + "/tfidf_nn_results"
# save_results(results, save_path)
loaded_results = load_results(save_path)
#display_loaded_results(loaded_results)

```

```{python}
#| label: nn tf accuracy plot
#| fig-cap: "An accuracy plot for the neural network model trained on the TF-IDF representation."

display_accuracy_plot(loaded_results)

```


```{python}
#| label: nn tf accuracies for the best fitted model
#| tbl-cap: "A table of accuracies for the neural network model trained on the TF-IDF representation."

display_table_nn(loaded_results)

```


```{python}
#| label: nn tf table
#| tbl-cap: "A table of results for the neural network model trained on the TF-IDF representation."

display_table_others(loaded_results)

```


```{python}
#| label: nn tf confusion matrix
#| fig-cap: "A confusion matrix for the neural network model trained on the TF-IDF representation."

display_confusion_matrix(loaded_results)

```


```{python}
#| label: nn tf test classification report
#| tbl-cap: "A table of test classification results for the neural network model trained on the TF-IDF representation."

display_test_classification_report(loaded_results)

```


### Support Vector Machine

```{python}

# svm_results_tf = train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed)

# save_model_params(svm_results_tf, os.path.join(save_directory, 'svm_tf_results'))

# svm_results_tf = load_model_params(os.path.join(save_directory, 'svm_tf_results'))

# results = visualize_and_verify_results(svm_results_tf, X_val_scaled, y_val_int, X_test_scaled, y_test_int, 'svm', class_labels=None)

# Save and load the results for later

save_path = save_directory2 + "/tfidf_svm_results"
# save_results(results, save_path)
loaded_results = load_results(save_path)
#display_loaded_results(loaded_results)

```

```{python}
#| label: svm tf accuracy plot
#| fig-cap: "An accuracy plot for the SVM model trained on the TF-IDF representation."

display_accuracy_plot(loaded_results)

```


```{python}
#| label: svm tf tuning table
#| tbl-cap: "A table of hyperparameter tuning results for the SVM model trained on the TF-IDF representation, with the best hyperparameters highlighted above."

display_table_others(loaded_results)

```


```{python}
#| label: svm tf confusion matrix
#| fig-cap: "A confusion matrix for the SVM model trained on the TF-IDF representation."

display_confusion_matrix(loaded_results)

```


```{python}
#| label: svm tf test classification report
#| tbl-cap: "A table of test classification results for the SVM model trained on the TF-IDF representation."

display_test_classification_report(loaded_results)

```

### Naive Bayes

```{python}

# nb_results_tf = train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test)

# save_model_params(nb_results_tf, os.path.join(save_directory, 'nb_tf_results'))

# nb_results_tf = load_model_params(os.path.join(save_directory, 'nb_tf_results'))

# results = visualize_and_verify_results(nb_results_tf, X_val, y_val, X_test, y_test, 'nb', class_labels=None)

# Save and load the results for later

save_path = save_directory2 + "/tfidf_nb_results"
# save_results(results, save_path)
loaded_results = load_results(save_path)
#display_loaded_results(loaded_results)

```


```{python}
#| label: nb tf accuracy plot
#| fig-cap: "An accuracy plot for the NB model trained on the TF-IDF representation."

display_accuracy_plot(loaded_results)

```


```{python}
#| label: nb tf tuning table
#| tbl-cap: "A table of hyperparameter tuning results for the NB model trained on the TF-IDF representation, with the best hyperparameters highlighted above."

display_table_others(loaded_results)

```


```{python}
#| label: nb tf confusion matrix
#| fig-cap: "A confusion matrix for the NB model trained on the TF-IDF representation."

display_confusion_matrix(loaded_results)

```


```{python}
#| label: nb tf test classification report
#| tbl-cap: "A table of test classification results for the NB model trained on the TF-IDF representation."

display_test_classification_report(loaded_results)

```


## Token Embeddings

Upon utilising text embedding as a representation technique alongside various models, a marked degradation in performance was observed compared to other preprocessing methods. With the feed-forward neural network, the training accuracy was a mere 0.409, while the validation accuracy dropped further to 0.369. The confusion matrix for test set predictions was quite telling: for a majority of sentences, the correct classes were not discerned. Intriguingly, the class "Zuma" was predominantly predicted. The test set showcased a precision of 0.367, recall of 0.368, and a notably lower f1 score of 0.328.

When paired with the support vector machines (SVM), post-tuning, the training accuracy stood at 0.406, with validation and test accuracies of 0.361 and 0.347, respectively. The precision was 0.342, the recall was 0.347, and the f1 score was slightly lower at 0.319.

Incorporating the Naive Bayes model with text embedding, post-optimisation, the training, validation, and test accuracies were 0.359, 0.359, and 0.338 in that order. This model's precision and recall registered at 0.335 and 0.338 respectively, with the test accuracy being considerably reduced to 0.291. This underlines the challenge posed by text embeddings in this specific context, as the results were notably inferior to other data preparation methods.

```{python}

x, y, _ = tokenize_text(sentence_data['sentence'], sentence_data['president'])
x_df = pd.DataFrame(x)

data = prepare_data(prepare_dataset, x_df, y)
X_train, y_train, X_val, y_val, X_test, y_test, input_dim, num_classes = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['X_test'], data['y_test'], data['input_dim'], data['num_classes']

# Scaled data for SVM
# scaler = cuml.preprocessing.StandardScaler()
# X_train_scaled = scaler.fit_transform(X_train)
# X_val_scaled = scaler.transform(X_val)
# X_test_scaled = scaler.transform(X_test)

# y_train_int = np.argmax(y_train, axis=1)
# y_val_int = np.argmax(y_val, axis=1)
# y_test_int = np.argmax(y_test, axis=1)

```

### Neural network

```{python}

# with tf.device('/device:GPU:0'):
#   nn_emb_tune = hyperparameter_search_nn(seed,
#                                         train_neural_network,
#                                         X_train, y_train,
#                                         X_val, y_val,
#                                         X_test, y_test)

# # Saving the model parameters
# save_model_params(nn_emb_tune, os.path.join(save_directory, 'nn_emb_results'))

# # Load the saved model parameters
# nn_emb_tune = load_model_params(os.path.join(save_directory, 'nn_emb_results'))

# results = visualize_and_verify_results(nn_emb_tune, X_val, y_val, X_test, y_test, 'nn', class_labels=None)

# Save and load the results for later

save_path = save_directory2 + "/emb_nn_results"
# save_results(results, save_path)
loaded_results = load_results(save_path)
#display_loaded_results(loaded_results)

```

```{python}
#| label: nn emb accuracy plot
#| fig-cap: "An accuracy plot for the neural network model trained on the text embeddings representation."

display_accuracy_plot(loaded_results)

```


```{python}
#| label: nn emb accuracies for the best fitted model
#| tbl-cap: "A table of accuracies for the neural network model trained on the text embeddings representation."

display_table_nn(loaded_results)

```


```{python}
#| label: nn emb table
#| tbl-cap: "A table of results for the neural network model trained on the text embeddings representation."

display_table_others(loaded_results)

```


```{python}
#| label: nn emb confusion matrix
#| fig-cap: "A confusion matrix for the neural network model trained on the text embeddings representation."

display_confusion_matrix(loaded_results)

```


```{python}
#| label: nn emb test classification report
#| tbl-cap: "A table of test classification results for the neural network model trained on the text embeddings representation."

display_test_classification_report(loaded_results)

```

### Support Vector Machine

```{python}

# svm_results_emb = train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed)

# save_model_params(svm_results_emb, os.path.join(save_directory, 'svm_emb_results'))

# svm_results_emb = load_model_params(os.path.join(save_directory, 'svm_emb_results'))

# results = visualize_and_verify_results(svm_results_emb, X_val_scaled, y_val_int, X_test_scaled, y_test_int, 'svm', class_labels=None)

# Save and load the results for later

save_path = save_directory2 + "/emb_svm_results"
# save_results(results, save_path)
loaded_results = load_results(save_path)
#display_loaded_results(loaded_results)

```

```{python}
#| label: svm emb accuracy plot
#| fig-cap: "An accuracy plot for the SVM model trained on the text embedding representation."

display_accuracy_plot(loaded_results)

```


```{python}
#| label: svm emb tuning table
#| tbl-cap: "A table of hyperparameter tuning results for the SVM model trained on the text embedding representation, with the best hyperparameters highlighted below."

display_table_others(loaded_results)

```


```{python}
#| label: svm emb confusion matrix
#| fig-cap: "A confusion matrix for the SVM model trained on the text embedding representation."

display_confusion_matrix(loaded_results)

```


```{python}
#| label: svm emb test classification report
#| tbl-cap: "A table of test classification results for the SVM model trained on the text embedding representation."

display_test_classification_report(loaded_results)

```

### Naive Bayes

```{python}

# nb_results_emb = train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test)

# save_model_params(nb_results_emb, os.path.join(save_directory, 'nb_emb_results'))

# nb_results_emb = load_model_params(os.path.join(save_directory, 'nb_emb_results'))

# results = visualize_and_verify_results(nb_results_emb, X_val, y_val, X_test, y_test, 'nb', class_labels=None)

# Save and load the results for later

save_path = save_directory2 + "/emb_nb_results"
# save_results(results, save_path)
loaded_results = load_results(save_path)
#display_loaded_results(loaded_results)

```

```{python}
#| label: nb emb accuracy plot
#| fig-cap: "An accuracy plot for the NB model trained on the text embedding representation."

display_accuracy_plot(loaded_results)

```


```{python}
#| label: nb emb tuning table
#| tbl-cap: "A table of hyperparameter tuning results for the NB model trained on the text embedding representation, with the best hyperparameters highlighted above."

display_table_others(loaded_results)

```


```{python}
#| label: nb emb confusion matrix
#| fig-cap: "A confusion matrix for the NB model trained on the text embedding representation."

display_confusion_matrix(loaded_results)

```


```{python}
#| label: nb emb test classification report
#| tbl-cap: "A table of test classification results for the NB model trained on the text embedding representation."

display_test_classification_report(loaded_results)

```

## BERT Embeddings with pre-trained classifier

The code for this section was adapted from the following source: [Google Tensorflow BERT tutorial](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)

Utilising the BERT embedding in tandem with a pre-trained model, a strategy known as transfer learning, distinctive patterns in performance were observed. Throughout the training epochs, the training accuracy showcased a consistent uptick. However, the validation accuracy plateaued rather swiftly, exhibiting minimal fluctuations thereafter. At the culmination of the training, the accuracy metrics stood as follows: training accuracy at 0.759, validation accuracy at 0.684, and a slightly higher test accuracy of 0.712. Further delving into the test set metrics, the precision was 0.71, recall was 0.707, and the f1 score was close behind at 0.708. An examination of the confusion matrix for the test set underscored these findings. The model predominantly made accurate predictions for the respective presidents, mirroring the positive metrics mentioned earlier. This highlights the efficacy of the BERT embeddings and transfer learning in this particular context, as the results were substantially more favourable than some other methods previously explored.

```{python}

y = sentence_data['president'].values

# Split dataset into train and a temporary set (70% - 30% split)
train_df, temp_df = train_test_split(sentence_data, test_size=0.3, stratify=y, random_state=seed)

# Split the temporary set into validation and test sets (50% - 50% split of the 30%)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['president'], random_state=seed)

# Assuming you have a list of unique presidents
unique_presidents = sentence_data['president'].unique()

# Create a mapping of president names to integer labels
label_map = {name: idx for idx, name in enumerate(unique_presidents)}

# Integer-encode the labels in the dataframes
train_df['president'] = train_df['president'].map(label_map)
val_df['president'] = val_df['president'].map(label_map)
test_df['president'] = test_df['president'].map(label_map)

# Convert pandas DataFrames to TensorFlow datasets
def df_to_tfdata(df, shuffle=True, batch_size=32):
    ds = tf.data.Dataset.from_tensor_slices((df["sentence"].values, df["president"].values))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(df))
    ds = ds.batch(batch_size)
    ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
    return ds

train_ds = df_to_tfdata(train_df)
val_ds = df_to_tfdata(val_df, shuffle=False)
test_ds = df_to_tfdata(test_df, shuffle=False)

```


```{python}

# Load the saved model
# import tensorflow_text as text
# saved_model_path = "saved_files/jared_sentences_bert"
# bert_model = tf.saved_model.load(saved_model_path)

# # Define a function to evaluate the model
# def evaluate_model(model, dataset):
#     predictions = []
#     true_labels = []

#     for inputs, labels in dataset:
#         logits = model(inputs, training=False)  # Forward pass
#         predicted_labels = np.argmax(logits, axis=1)
#         true_labels.extend(labels.numpy())
#         predictions.extend(predicted_labels)

#     return true_labels, predictions



```

```{python}

def save_output(train_true, train_pred, val_true, val_pred, test_true, test_pred, filename):
    with open(filename, 'wb') as f:
        pickle.dump((train_true, train_pred, val_true, val_pred, test_true, test_pred), f)

def load_output(filename):
    with open(filename, 'rb') as f:
        train_true, train_pred, val_true, val_pred, test_true, test_pred = pickle.load(f)
    return train_true, train_pred, val_true, val_pred, test_true, test_pred


```

```{python}

# # Evaluate the model on the training, validation, and test datasets
# train_true, train_pred = evaluate_model(bert_model, train_ds)
# val_true, val_pred = evaluate_model(bert_model, val_ds)
# test_true, test_pred = evaluate_model(bert_model, test_ds)

# # Save the outputs
# save_output(train_true, train_pred, val_true, val_pred, test_true, test_pred, 'bert_evaluation.pkl')

train_true, train_pred, val_true, val_pred, test_true, test_pred = load_output('bert_evaluation.pkl')


```


```{python}
# Display the saved image
display(Image(filename='saved_files/train_val_curves_bert.png'))

```

```{python}

def extract_metrics(y_true, y_pred):
    accuracy = np.mean(np.array(y_true) == np.array(y_pred))
    
    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)
    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)
    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)
    
    metrics = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }
    return metrics

train_metrics = extract_metrics(train_true, train_pred)
val_metrics = extract_metrics(val_true, val_pred)
test_metrics = extract_metrics(test_true, test_pred)

# Create a DataFrame for a neat table
df = pd.DataFrame([train_metrics, val_metrics, test_metrics], 
                  index=['Training', 'Validation', 'Test'])

# Round the values to 3 decimal places for better presentation
df = df.round(3)

# Display the table
display(df)


```


```{python}

# Function to plot confusion matrix
def plot_confusion_matrix(true, pred, title):
    matrix = confusion_matrix(true, pred)
    plt.figure(figsize=(10, 7))
    sns.heatmap(matrix, annot=True, fmt="d", cmap="Blues",
                xticklabels=set(true), yticklabels=set(true))
    plt.title(title)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

# Plot confusion matrices
plot_confusion_matrix(test_true, test_pred, title="Test Data Confusion Matrix")

```

# Discussion:

This study aimed to figure out which of the South African presidents, from 1994 to 2022, might have said certain sentences during their State of the Nation Address (SONA). Different ways of processing the text, such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (tf-idf), and text embeddings, were used. These methods were then paired with machine learning models to see which combination worked best.

With the Bag of Words (BoW) method, the feed-forward neural network did well in training but not as well in validation, suggesting it might not do well with new, unseen data. The SVM and Naive Bayes models had similar outcomes. The tf-idf method gave results close to BoW for the neural net and SVM, but Naive Bayes seemed a bit more stable. However, simple text embeddings didn't work as well across the board. This could be because these embeddings might be too basic to capture the unique way presidents speak in their SONA addresses.

On the other hand, using BERT embeddings with a pre-trained model gave us some hope. The model kept getting better during training, and its test results were the best among all the models. This suggests that using advanced methods like BERT might be the way forward for such tasks.

# Conclusion:

This study shows how important it is to pick the right method to process text and the right model to analyse it. While methods like BoW and tf-idf gave decent results, simple text embeddings didn't do as well. But, the combination of BERT embeddings and a pre-trained model stood out.

This has two main takeaways. First, for researchers looking into political speeches, these models can help in figuring out who might have said an unattributed speech. Second, for those into machine learning, it highlights the growing role of advanced methods like BERT.

Looking ahead, it might be worth exploring even better text processing methods or fine-tuning models like BERT for even more accurate results. Overall, this study shows the exciting possibilities when combining tech with the study of political speeches.